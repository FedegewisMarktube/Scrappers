import os
import time
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

BASE_URL = "https://ar.computrabajo.com"
BA_LIST_URL = f"{BASE_URL}/empleos-en-mendoza"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "es-AR,es;q=0.9,en;q=0.8",
}

# üëá NUEVO: base del repo
BASE_DIR = os.path.dirname(__file__)
HTML_DIR = os.path.join(BASE_DIR, "..", "data", "mendoza")

CSS_DIR = os.path.join(BASE_DIR, "..", "data", "CSS")



def ensure_dirs():
    os.makedirs(HTML_DIR, exist_ok=True)
    os.makedirs(CSS_DIR, exist_ok=True)


def get_html(url: str) -> str | None:
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
    except requests.RequestException as e:
        print(f"[ERROR RED] {url}: {e}")
        return None

    if resp.status_code != 200:
        print(f"[ERROR HTTP {resp.status_code}] {url}")
        return None

    return resp.text


def save_html(content: str, filename: str) -> None:
    path = os.path.join(HTML_DIR, filename)
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"[HTML] Guardado {path}")


def sanitize_filename(name: str) -> str:
    # para nombres de archivo simples
    return "".join(c if c.isalnum() or c in "-_." else "_" for c in name)


def download_css_from_html(html: str, page_url: str, downloaded: set) -> None:
    soup = BeautifulSoup(html, "html.parser")
    links = soup.find_all("link", rel="stylesheet")

    for link in links:
        href = link.get("href")
        if not href:
            continue

        css_url = urljoin(page_url, href)
        if css_url in downloaded:
            continue

        print(f"[CSS] Descargando {css_url}")
        try:
            r = requests.get(css_url, headers=HEADERS, timeout=20)
            if r.status_code != 200:
                print(f"   -> Error HTTP {r.status_code} en CSS")
                continue
        except requests.RequestException as e:
            print(f"   -> Error red CSS: {e}")
            continue

        parsed = urlparse(css_url)
        filename = sanitize_filename(os.path.basename(parsed.path) or "style.css")
        path = os.path.join(CSS_DIR, filename)

        with open(path, "wb") as f:
            f.write(r.content)
        print(f"   -> Guardado {path}")

        downloaded.add(css_url)


def scrape_home(downloaded_css: set) -> None:
    print("\n=== HOME ===")
    url = BASE_URL
    html = get_html(url)
    if not html:
        print("No se pudo descargar el home.")
        return

    save_html(html, "home.html")
    download_css_from_html(html, url, downloaded_css)


def scrape_mendoza_pages(downloaded_css: set, delay: float = 2.0, max_pages: int | None = None) -> None:
    print("\n=== P√ÅGINAS MENDOZA ===")
    page = 1

    while True:
        if max_pages is not None and page > max_pages:
            print("Max de p√°ginas alcanzado, corto.")
            break

        url = f"{BA_LIST_URL}?p={page}"
        print(f"\n[PAGE] {page} -> {url}")
        html = get_html(url)
        if not html:
            print("No se pudo descargar la p√°gina, corto.")
            break

        # Guardar HTML de esta p√°gina
        save_html(html, f"mendoza_p{page}.html")
        download_css_from_html(html, url, downloaded_css)

        # Heur√≠stica simple para cortar: si ya no hay resultados nuevos
        soup = BeautifulSoup(html, "html.parser")
        job_cards = soup.find_all("article")
        if not job_cards:
            print("No se encontraron m√°s avisos (no hay <article>), corto.")
            break

        page += 1
        time.sleep(delay)


if __name__ == "__main__":
    ensure_dirs()
    downloaded_css = set()

    # 1) Home
    scrape_home(downloaded_css)

    # 2) Todas las p√°ginas de empleos en Buenos Aires
    #    Mientras prob√°s pod√©s usar max_pages=3, despu√©s lo pon√©s en None
    scrape_mendoza_pages(downloaded_css, delay=2.0, max_pages=30)

print("\nListo. HTML en 'empleobusca.com/data/mendoza' y CSS en 'empleobusca.com/data/css_mendoza'.")
